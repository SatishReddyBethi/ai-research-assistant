{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd3731-d2b3-4d16-b157-6e08720a9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "from huggingface_hub import login as hf_login\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c225f-4333-46e5-9d35-9078e8cce018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_print(message: str):\n",
    "    \"\"\"\n",
    "    Custom print function for consistent logging.\n",
    "    Args:\n",
    "        message (str): The message to print.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    prefix = \"-> \"\n",
    "    if message.startswith(\"\\n\"):\n",
    "        prefix = \"\\n-> \"\n",
    "        message = message[1:]\n",
    "    print(f\"{prefix}{message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Helper function to format a list of Document objects into a single string.\n",
    "    Args:\n",
    "        docs (List[Document]): List of Document objects.\n",
    "    Returns:\n",
    "        str: Concatenated string of document contents.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adc611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_research_papers(data_path: str, print_logs: bool = False):\n",
    "    \"\"\"\n",
    "    Load and split research papers from the specified directory.\n",
    "    Args:\n",
    "        data_path (str): The path to the directory containing research papers in PDF format.\n",
    "        print_logs (bool): Whether to print logs during the process.\n",
    "    Returns:\n",
    "        List[Document]: A list of Document objects representing the split chunks of the research papers.\n",
    "    \"\"\"\n",
    "    all_docs_data = []\n",
    "\n",
    "    if print_logs:\n",
    "        c_print(f\"Loading research papers from '{data_path}' folder...\")\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(data_path):\n",
    "        for file in filenames:\n",
    "            full_file_path = os.path.join(dirpath, file)\n",
    "            loader = PyPDFLoader(full_file_path)\n",
    "            docs = loader.load()\n",
    "            # The.load() method returns a list of Document objects (one for each page).\n",
    "            # We use.extend() to add all pages from the current PDF to our main list.\n",
    "            all_docs_data.extend(docs)\n",
    "            if print_logs:\n",
    "                c_print(f\"\\n{len(docs)} Pages loaded from: {full_file_path}\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    if print_logs:\n",
    "        c_print(f\"\\nSplitting {len(all_docs_data)} pages into chunks...\")\n",
    "    \n",
    "    splits = text_splitter.split_documents(all_docs_data)\n",
    "    \n",
    "    if print_logs:\n",
    "        c_print(f\"Created {len(splits)} chunks.\")\n",
    "        # c_print(f\"\\nContents of a single chunk:\\n{splits[0].page_content}\")\n",
    "        # c_print(f\"\\nMetadata of the chunk:\\n{splits[0].metadata}\")    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6903a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_vector_store(model_name: str = \"all-MiniLM-L6-v2\", device: str = \"cpu\", persist_directory: str = \".chromaDB\", print_logs: bool = False, verify_vector_store: bool = False):\n",
    "    \"\"\"\n",
    "    Create or load the vector store from research papers.\n",
    "    Args:\n",
    "        model_name (str): The name of the embedding model to use. By default, we use 'all-MiniLM-L6-v2' as it is a popular and efficient model that runs locally.\n",
    "        device (str): The device to run the embedding model on. Options are 'cpu', 'cuda', or 'xpu' (for Intel XPU).\n",
    "        persist_directory (str): The directory where the vector store is persisted.\n",
    "        print_logs (bool): Whether to print logs during the process.\n",
    "        test_vector_store (bool): Whether to perform a test query on the vector store after loading/creation.\n",
    "    Returns:\n",
    "        vectorstore (Chroma): The loaded or newly created vector store.\n",
    "    \"\"\"\n",
    "\n",
    "    if print_logs:\n",
    "        c_print(\"\\nInitializing embedding model...\")\n",
    "\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name,\n",
    "        model_kwargs = {'device': device} # Use 'cuda' if you have a GPU\n",
    "    )\n",
    "\n",
    "    if os.path.exists(persist_directory):\n",
    "        if print_logs:\n",
    "            c_print(f\"Loading existing vector store from '{persist_directory}' folder...\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embedding_model\n",
    "        )\n",
    "        if print_logs:\n",
    "            c_print(\"Vector store loaded successfully.\")\n",
    "    else:\n",
    "        splits = load_research_papers(data_path=\"./data\", print_logs=print_logs)\n",
    "        if print_logs:\n",
    "            c_print(\"\\nNo existing vector store found.\")\n",
    "            c_print(f\"Creating new vector store in '{persist_directory}' folder...\")\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        if print_logs:\n",
    "            c_print(\"New vector store created and persisted.\")\n",
    "    \n",
    "    if verify_vector_store:\n",
    "        # Check if the vector store is working correctly\n",
    "        c_print(\"\\n--- Testing the vector store with a similarity search ---\")\n",
    "        test_query = \"What is the main contribution of the 'Exergames for telerehabilitation' thesis?\"\n",
    "        retrieved_docs = vectorstore.similarity_search(test_query, k=2) # Retrieve the top 2 most relevant chunks\n",
    "\n",
    "        c_print(f\"Query: '{test_query}'\")\n",
    "        c_print(f\"Retrieved {len(retrieved_docs)} documents.\")\n",
    "\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            c_print(f\"\\n--- Document {i+1}: ---\")\n",
    "            c_print(doc.page_content)\n",
    "            c_print(\"\\n--- Metadata: ---\")\n",
    "            c_print(doc.metadata)\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b236212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_id: str = \"google/gemma-2b-it\", device:str = \"cpu\", hf_env_var: str = \"HUGGINGFACE_API_KEY\", print_logs: bool = False):\n",
    "    \"\"\"\n",
    "    Load the local LLM using Hugging Face transformers and wrap it in a LangChain HuggingFacePipeline.\n",
    "    Args:\n",
    "        model_id (str): The Hugging Face model ID to load. By default, we use 'google/gemma-2b-it' as it's powerful and runs locally.\n",
    "        device (str): The device to run the model on. Options are 'cpu', 'cuda', or 'xpu' (for Intel XPU).\n",
    "        hf_env_var (str): The environment variable name that contains the Hugging Face API token.\n",
    "        print_logs (bool): Whether to print logs during the process.\n",
    "    Returns:\n",
    "        llm (HuggingFacePipeline): The loaded local LLM wrapped in a LangChain HuggingFacePipeline.\n",
    "    \"\"\"\n",
    "    # Get Hugging Face token from environment variable\n",
    "    hf_token=os.getenv(hf_env_var)\n",
    "\n",
    "    # Login to Hugging Face. Hugging Faces should automatically show a login prompt if needed.\n",
    "    hf_login(token=hf_token)\n",
    "\n",
    "    if print_logs:\n",
    "        c_print(f\"\\nInitializing local LLM ({model_id})...\")\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # Format device string for transformers\n",
    "    if device == \"xpu\" or device == \"cuda\":\n",
    "        device = \"xpu:0\"\n",
    "    else:\n",
    "        device = \"auto\"\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=torch.bfloat16, # Same range as float32 but less precision, saves memory\n",
    "        device_map=device # Automatically use GPU if available\n",
    "    )\n",
    "\n",
    "    # Create a LangChain text-generation pipeline from the transformers library\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512, # The maximum number of tokens to generate\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    # Wrap the transformers pipeline in a LangChain object\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    if print_logs:\n",
    "        c_print(\"LLM initialized successfully.\")\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ae5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_chain(llm, print_logs: bool = False):\n",
    "    \"\"\"\n",
    "    Build the RAG chain using the provided vector store and LLM.\n",
    "    Args:\n",
    "        llm (HuggingFacePipeline): The local LLM for generating answers.\n",
    "        print_logs (bool): Whether to print logs during the process.\n",
    "    Returns:\n",
    "        rag_chain: The constructed RAG chain.\n",
    "    \"\"\"\n",
    "    # This prompt is instructs the LLM to answer questions 'only' based on\n",
    "    # the provided context from the papers. This is a key to prevent hallucinations.\n",
    "    template = \"\"\"\n",
    "    You are an expert research assistant. Your task is to answer questions based on the provided context.\n",
    "    Answer the question based only on the following context.\n",
    "    If the answer cannot be found in the context, write \"I am sorry! I could not find the answer in the provided documents.\n",
    "    If you find multiple answers, combine them into a comprehensive response.\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"context\") | RunnableLambda(format_docs),\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | prompt | llm | StrOutputParser())\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_pipeline(rag_chain, query, retriever, stream_output:bool = True, print_logs: bool = False, print_sources: bool = False):\n",
    "    \"\"\"\n",
    "    Run the RAG pipeline with the given query.\n",
    "    Args:\n",
    "        rag_chain: The RAG chain to use for answering the query.\n",
    "        query (str): The user's question to answer.\n",
    "        retriever: The document retriever.\n",
    "        stream_output (bool): Whether to stream the output token by token.\n",
    "        print_logs (bool): Whether to print logs during the process.\n",
    "        print_sources (bool): Whether to print the source documents used for answering.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if print_logs:\n",
    "        c_print(f\"Query: {query}\\n\")\n",
    "        c_print(\"Retrieving relevant documents...\")\n",
    "    \n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "    full_response = \"\"\n",
    "    chain_input = {\"context\": retrieved_docs, \"question\": query}\n",
    "\n",
    "    if stream_output:\n",
    "        if print_logs:\n",
    "            c_print(\"Answer (Streaming):\")        \n",
    "        # The.stream() method returns a generator that yields tokens as they are generated.\n",
    "        for chunk in rag_chain.stream(chain_input):\n",
    "            if print_logs:\n",
    "                # Print each chunk as it arrives\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "            full_response += chunk\n",
    "    else:\n",
    "        if print_logs:\n",
    "            c_print(\"Answer:\")        \n",
    "        full_response = rag_chain.invoke({\"question\": query})\n",
    "        print(full_response)\n",
    "\n",
    "    if print_logs and print_sources:\n",
    "        # This can be printed before answer generation (if needed)\n",
    "        c_print(\"\\nSources: ---\")\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            c_print(f\"Source {i+1} (from '{doc.metadata.get('source', 'N/A')}', page {doc.metadata.get('page', 'N/A')}):\")\n",
    "            print(f\"\\\"{doc.page_content[:250]}...\\\"\\n\")\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_logs = True\n",
    "print_sources = False\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.xpu.is_available():\n",
    "    # Check if Intel XPU (GPU) is available\n",
    "    device = \"xpu\"\n",
    "else:\n",
    "    device =  \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "vectorstore = create_or_load_vector_store(device=device, print_logs=print_logs)\n",
    "llm = load_llm(device=device, print_logs=print_logs)\n",
    "rag_chain = build_rag_chain(llm, print_logs=print_logs)\n",
    "# Check if the RAG chain is working as expected\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "query = \"What is the purpose of the RehabFork system described in the papers?\"\n",
    "response = run_rag_pipeline(rag_chain, query, retriever, print_logs=print_logs, print_sources=print_sources)\n",
    "if not print_logs:\n",
    "    print(f\"Q: {query}\\nA: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
