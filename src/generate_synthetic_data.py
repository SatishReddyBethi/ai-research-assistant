import json
import os
import random
from tqdm import tqdm
from q_and_a_rag_model import load_research_papers, load_llm, build_q_and_a_rag_chain
from utils import CustomPrinter, get_device, get_env

def generate_synthetic_data(splits, llm, dataset_file_path, num_examples: int = 200, replace_existing_file: bool = False, c_print = print):
    """
    Generate a synthetic dataset for fine-tuning a language model to perform
    high-quality, academic-style summarization of research paper paragraphs.
    Args:
        splits (List[Document]): List of document chunks to sample from.
        llm: The local language model instance used for generation.
        dataset_file_path (str): Path to save the generated dataset.
        num_examples (int): Number of examples to generate for the dataset.
        replace_existing_file (bool): Whether to replace the existing dataset file if it exists.
        c_print: CustomPrinter instance for logging.
    Returns:
        None
    """
    # TASK_DESCRIPTION = """
    # The model will be fine-tuned to perform high-quality, academic-style summarization.
    # Given a paragraph from a research paper (the 'input'), the model should generate
    # a 2-3 sentence summary (the 'output') that is concise, formal, and suitable for
    # inclusion in a scientific abstract.
    # """

    SYNTHETIC_DATA_GENERATION_PROMPT_TEMPLATE = """
    You are an expert academic writer and researcher specializing in engineering and rehabilitation technology.
    Your task is to summarize the following paragraph from a research paper.

    **Instructions:**
    1.  Read the paragraph carefully to understand its core contribution.
    2.  Generate a summary that is exactly **two to three sentences** long.
    3.  The summary must maintain a formal, scientific tone suitable for a publication's abstract.
    4.  Focus on the key methods, results, or conclusions presented in the paragraph.
    5.  Do not add any information that is not present in the original paragraph.

    **Paragraph to Summarize:**
    "{input_paragraph}"

    **Expert Summary:**
    """

    # TARGET_DATA_FORMAT_EXAMPLE = {
    #     "input": "The full text of a paragraph from one of the research papers...",
    #     "output": "The expert-level, 2-3 sentence summary generated by the LLM using the prompt above."
    # }

    # Check if dataset already exists to avoid re-generating
    if os.path.exists(dataset_file_path):
        c_print(f"Dataset file already exists at '{dataset_file_path}'. Skipping generation.")
        # Optional: Load and inspect the existing dataset
        with open(dataset_file_path, 'r') as f:
            existing_data = [json.loads(line) for line in f]
        c_print(f"Loaded {len(existing_data)} examples from the existing file.")
    else:
        c_print(f"Generating {num_examples} examples for the fine-tuning dataset...")
        
    # Randomly sample diverse chunks from the documents
    # This ensures our training data is not biased towards the beginning of the papers.
    if len(splits) < num_examples:
        raise ValueError(f"Not enough document splits ({len(splits)}) to generate {num_examples} examples.")

    sampled_splits = random.sample(splits, num_examples)

    # --- 6. Generate the dataset using a progress bar ---
    generated_dataset = []
    first_sample = True

    # tqdm adds a progress bar to the loop
    for split in tqdm(sampled_splits, desc="Generating Summaries"):
        input_paragraph = split.page_content
        
        # Format the prompt
        prompt_text = SYNTHETIC_DATA_GENERATION_PROMPT_TEMPLATE.format(input_paragraph=input_paragraph)
        
        # Generate the summary using the local LLM
        raw_summary = llm.invoke(prompt_text)
        
        # Remove initial promt from the generated output
        cleaned_summary = raw_summary.split("**Expert Summary:**")[-1].strip()
        
        # Create the training example
        training_example = {
            "input": input_paragraph,
            "output": cleaned_summary
        }

        generated_dataset.append(training_example)
        
        if first_sample:
            # Show an example of the generated training data
            c_print("\n--- Example Generated Training Example: ---")
            c_print(json.dumps(training_example, indent=2))
            first_sample = False
        

    # Save the dataset to a file
    with open(dataset_file_path, 'w') as f:
        for example in generated_dataset:
            f.write(json.dumps(example) + '\n')
            
    c_print(f"\nDataset generation complete. Saved {len(generated_dataset)} examples to '{dataset_file_path}'.")

    # Verify the result by inspecting the first example
    c_print("\n--- Example from generated dataset: ---")
    with open(dataset_file_path, 'r') as f:
        first_line = f.readline()
        c_print(json.dumps(json.loads(first_line), indent=2))

if __name__ == "__main__":
    print_logs = True
    print_sources = False
    num_examples=200
    replace_existing_dataset=True
    c_print = CustomPrinter()
    device = get_device()
    c_print(f"Using device: {device}")
    use_xpu = get_env("USE_XPU")

    if device == "xpu" and not use_xpu:
        device = "cpu"
        print(f"USE_XPU is set to {use_xpu} so switching device to 'cpu'")

    if device == "xpu":
        # import ipex even if not used as it loads all the required optimization for XPU
        import intel_extension_for_pytorch as ipex

    data_path = get_env("DATA_DIR_PATH")
    splits = load_research_papers(data_path=data_path, print_logs=print_logs, c_print=c_print)
    llm = load_llm(device=device, print_logs=print_logs, c_print=c_print)
    rag_chain = build_q_and_a_rag_chain(llm, print_logs=print_logs, c_print=c_print)
    generate_synthetic_data(splits, llm, "fine_tuning_dataset_2.jsonl", num_examples=num_examples, replace_existing_file=replace_existing_dataset, c_print=c_print)
    